{
 "cells": [
  {
   "cell_type": "raw",
   "id": "015ddb1f-5635-492d-bcd9-d79489015df3",
   "metadata": {},
   "source": [
    "CC-PRAM\n",
    "\n",
    "Used Kernel: \n",
    "- eval_semantics\n",
    "\n",
    "Used Conda:\n",
    "- conda activate kmeans_env\n",
    "\n",
    "Sources: \n",
    "- adwanced k-means https://towardsdatascience.com/advanced-k-means-controlling-groups-sizes-and-selecting-features-a998df7e6745\n",
    "- cosine similarity for kmeans using l2 normalized embeddings\n",
    "\n",
    "pip install protobuf==3.20.3\n",
    "pip install jax libclang tensorflow-io-gcs-filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c831076-15bc-4a93-b958-fb68d9651370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from k_means_constrained import KMeansConstrained\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771b991-e5b3-4999-9459-52f3b0df49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2818e-6ec0-4191-a5cd-13992be034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to get embedding for a single text\n",
    "def get_single_text_embedding(text, tokenizer, model, device):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    inputs = tokenizer(cleaned_text, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(axis=1).cpu().numpy()\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(embedding, fix_embedding):\n",
    "    return cosine_similarity([embedding], [fix_embedding])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a85e-9bfa-42d3-a7e9-92d4d837cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BlueBERT tokenizer and model\n",
    "model_name = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b077-7bf9-4821-b1f8-f1f0d6e92773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Cleaned DataFrame and Ensure Correct Data Types\n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the cleaned DataFrame\n",
    "df_loaded = pd.read_csv('df_cc-pram_icd-title_mapped_cleaned.csv')\n",
    "\n",
    "# Ensure subject_id and stay_id are strings\n",
    "df_loaded['subject_id'] = df_loaded['subject_id'].astype(str)\n",
    "df_loaded['stay_id'] = df_loaded['stay_id'].astype(str)\n",
    "\n",
    "# Extract relevant IDs for fetching data from MongoDB\n",
    "subject_ids = df_loaded['subject_id'].unique().tolist()\n",
    "stay_ids = df_loaded['stay_id'].unique().tolist()\n",
    "\n",
    "if 'Unnamed: 0' in df_loaded.columns:\n",
    "    df_loaded = df_loaded.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(\"Loaded DataFrame with shape:\", df_loaded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa05014-0ee9-4a53-88cd-206182e59ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745667b-4bf4-4851-aeae-d43d0f7f9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fetch Corresponding Data from MongoDB\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Adjust the connection string as necessary\n",
    "db = client['MIMIC-IV']  # Replace with your database name\n",
    "collection = db['ED-VitalSigns']  # Replace with your collection name\n",
    "\n",
    "# Fetch the data from MongoDB\n",
    "query = {\"subject_id\": {\"$in\": subject_ids}, \"stay_id\": {\"$in\": stay_ids}}\n",
    "projection = {\"_id\": 0}  # Exclude only the _id field, include all others\n",
    "\n",
    "data = list(collection.find(query, projection))\n",
    "df_vitalsigns = pd.DataFrame(data)\n",
    "\n",
    "print(\"Fetched data from MongoDB with shape:\", df_vitalsigns.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d4e21-29e3-426c-a408-98945426ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vitalsigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffbec5-01c5-4396-869c-1b4da0c991d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vitalsigns_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c40fd-2948-40ea-8c6a-dfe5e60de03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with NaN values in specific columns\n",
    "columns_to_check = ['charttime', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain']\n",
    "df_vitalsigns_cleaned = df_vitalsigns.dropna(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a005b4-fe38-4f57-9b3c-cbcbffde42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetched data from MongoDB with shape:\", df_vitalsigns_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99bc0d-dc4d-420a-b877-4b14a4946c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vitalsigns_cleaned.to_csv('df_cc-pram_vitalsigns_cleaned.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c96b4b-99e4-4303-a3d1-0c2493f87ce0",
   "metadata": {},
   "source": [
    "# Clustering for PRAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5d46a-d98c-410d-ad25-bba609d0351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from df based name\n",
    "df_sample = df_pyxis.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Check the sample size after removing duplicates\n",
    "sample_size = len(df_sample)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fce12-8dfd-4d03-bdb4-d4243d7b50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the sample\n",
    "df_sample = df_sample.copy()  # Create a proper copy to avoid SettingWithCopyWarning\n",
    "df_sample.loc[:, 'name_embedding'] = df_sample['name'].apply(get_single_text_embedding, args=(tokenizer, model, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "\n",
    "# Stack the embeddings into a 2D array for clustering\n",
    "embeddings = np.vstack(df_sample['name_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87becb4-184c-4f95-a692-f2ebc90cf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction and plot explained variance\n",
    "pca_inspect = PCA(n_components=100)\n",
    "reduced_embeddings_inspect = pca_inspect.fit_transform(embeddings)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = np.cumsum(pca_inspect.explained_variance_ratio_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid()\n",
    "plt.savefig('pca_explained_variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4472da-b5dc-4eb3-8ef3-64f767fdfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of components based on explained variance\n",
    "n_components = 45  # Adjust this based on the explained variance plot (e.g. which n makes out 80% of variance)\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "df_sample['reduced_embeddings'] = list(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b490df2-0cc2-4a5e-90b5-6706f725c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the PCA-reduced embeddings using L2 norm\n",
    "# L2 normalization scales each vector to have a unit norm (length of 1). This is essential because it ensures that\n",
    "# the distance metric (cosine similarity) focuses on the direction of the vectors rather than their magnitudes.\n",
    "# This step makes the Euclidean distance between normalized vectors equivalent to cosine similarity, which is \n",
    "# crucial for clustering methods like k-means that rely on distance measures.\n",
    "normalized_embeddings = normalize(reduced_embeddings, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4ff25-36c4-4269-8905-89c57c7d9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SEEEMS TO WORK\n",
    "df_sample.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07559897-3007-4129-803f-7809f23d67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Pick best_k within range of since \"The product of size_max and n_clusters must be larger than or equal the number of samples (X):\"')\n",
    "print(sample_size)\n",
    "print(sample_size/10)\n",
    "print(sample_size/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45e9f-284a-4187-b046-776b238287df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters and the constraints\n",
    "\n",
    "size_min = 5  \n",
    "size_max = 10 \n",
    "best_k = 50\n",
    "\n",
    "# Perform K-means constrained clustering\n",
    "kmeans_constrained = KMeansConstrained(\n",
    "    n_clusters=best_k,\n",
    "    size_min=size_min,\n",
    "    size_max=size_max,\n",
    "    init=\"k-means++\",\n",
    "    n_init=200,\n",
    "    max_iter=1000,\n",
    "    random_state=1984\n",
    ")\n",
    "cosine_clusters = kmeans_constrained.fit_predict(normalized_embeddings)\n",
    "\n",
    "df_sample['cosine_cluster_name'] = cosine_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63cf8c-3d6a-43cd-997c-9e240c77eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine distance of each point to its respective cluster center and add the values for a distinct marker within cluster, which can be used in PRAM (Post Randomization)\n",
    "cosine_distances = np.zeros(normalized_embeddings.shape[0])\n",
    "for i in range(normalized_embeddings.shape[0]):\n",
    "    cluster_center = kmeans_constrained.cluster_centers_[cosine_clusters[i]]\n",
    "    cosine_similarity_value = np.dot(normalized_embeddings[i], cluster_center)\n",
    "    cosine_distances[i] = 1 - cosine_similarity_value\n",
    "\n",
    "df_sample['cosine_distance_to_center_name'] = cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8793d1-8acc-49a0-a483-bd268f48d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c1719-9a1a-4428-94c9-c2cd6d3f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyxis.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a5ebb-08a4-4687-9813-d0fe875cecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from df_sample for name clustering\n",
    "cluster_mapping = df_sample.set_index('name')['cosine_cluster_name'].to_dict()\n",
    "distance_mapping = df_sample.set_index('name')['cosine_distance_to_center_name'].to_dict()\n",
    "\n",
    "# Map the cluster assignments and distances back to the original DataFrame\n",
    "df_mapped = df_pyxis.copy()\n",
    "df_mapped['cosine_cluster_name'] = df_pyxis['name'].map(cluster_mapping)\n",
    "df_mapped['cosine_distance_to_center_name'] = df_pyxis['name'].map(distance_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65ae42-c06d-46f8-9810-882b5efdbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c8c99-5404-482f-a121-4075473abd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean and standard deviation for each cluster\n",
    "cluster_stats = df_mapped.groupby('cosine_cluster_name')['cosine_distance_to_center_name'].agg(['mean', 'std'])\n",
    "\n",
    "# Define a function to identify high outliers\n",
    "def is_high_outlier(row):\n",
    "    cluster = row['cosine_cluster_name']\n",
    "    distance = row['cosine_distance_to_center_name']\n",
    "    mean = cluster_stats.loc[cluster, 'mean']\n",
    "    std = cluster_stats.loc[cluster, 'std']\n",
    "    return distance > (mean + 3 * std)\n",
    "\n",
    "# Check if a cluster is valid based on unique counts, since we aim to use PRAM with a 3x3 matrix at least.\n",
    "def is_valid_cluster(cluster, df, min_unique_names=3, min_unique_distances=3):\n",
    "    names_count = df[df['cosine_cluster_name'] == cluster]['name'].nunique()\n",
    "    distances_count = df[df['cosine_cluster_name'] == cluster]['cosine_distance_to_center_name'].nunique()\n",
    "    return names_count >= min_unique_names and distances_count >= min_unique_distances\n",
    "\n",
    "# Identify valid clusters\n",
    "valid_clusters = [cluster for cluster in df_mapped['cosine_cluster_name'].unique() if is_valid_cluster(cluster, df_mapped)]\n",
    "\n",
    "# Filter df_mapped to include only valid clusters\n",
    "df_valid = df_mapped[df_mapped['cosine_cluster_name'].isin(valid_clusters)].copy()\n",
    "\n",
    "# Apply the outlier detection to valid clusters\n",
    "df_valid['is_high_outlier'] = df_valid.apply(is_high_outlier, axis=1)\n",
    "\n",
    "# Determine the highest value within each cluster for these outliers\n",
    "highest_outliers = df_valid[df_valid['is_high_outlier']].groupby('cosine_cluster_name')['cosine_distance_to_center_name'].transform('max')\n",
    "\n",
    "# Add 'to_be_removed' column to mark the highest outliers\n",
    "df_valid['to_be_removed'] = df_valid.apply(\n",
    "    lambda row: row['is_high_outlier'] and row['cosine_distance_to_center_name'] == highest_outliers[row.name],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create a cleaned DataFrame without the specific highest outliers\n",
    "df_mapped_cleaned = df_valid[~df_valid['to_be_removed']].copy()\n",
    "\n",
    "# Track affected clusters\n",
    "affected_clusters = df_valid.loc[df_valid['to_be_removed'], 'cosine_cluster_name'].unique()\n",
    "\n",
    "# Drop the extra columns used for outlier detection\n",
    "df_mapped_cleaned.drop(['is_high_outlier', 'to_be_removed'], axis=1, inplace=True)\n",
    "\n",
    "# Print affected clusters\n",
    "print(\"Affected clusters with highest outlier removed:\", affected_clusters)\n",
    "\n",
    "# Verify that all clusters in df_mapped_cleaned have at least 3 unique `name` and `cosine_distance_to_center_name` values\n",
    "cluster_validity_cleaned = df_mapped_cleaned.groupby('cosine_cluster_name').agg({\n",
    "    'name': 'nunique',\n",
    "    'cosine_distance_to_center_name': 'nunique'\n",
    "})\n",
    "\n",
    "invalid_clusters = cluster_validity_cleaned[(cluster_validity_cleaned['name'] < 3) | (cluster_validity_cleaned['cosine_distance_to_center_name'] < 3)].index\n",
    "\n",
    "if len(invalid_clusters) == 0:\n",
    "    print(\"\\nAll clusters in df_mapped_cleaned have at least 3 unique `name` and `cosine_distance_to_center_name` values.\")\n",
    "else:\n",
    "    print(\"\\nClusters violating the rule (less than 3 uniques):\", list(invalid_clusters))\n",
    "\n",
    "# Print change in size before and after cleaning\n",
    "print(f\"\\nSize before cleaning: {df_loaded.shape[0]}\")\n",
    "print(f\"Size after cleaning and mapping: {df_mapped_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e496f-2def-46ca-935d-f5fdc542f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 14\n",
    "df_cluster_sorted = df_mapped_cleaned[df_mapped_cleaned['cosine_cluster_name'] == cluster].sort_values(by='charttime', ascending=False).drop_duplicates(subset='name')\n",
    "df_cluster_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bbe35-faac-4671-a150-624607add0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyxis.to_csv('df_cc-pram_pyxisname-original_triage_sample.csv', index=True)\n",
    "df_mapped_cleaned.to_csv('df_cc-pram_pyxisname_mapped_cleaned.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8139cc2-8e5f-4c8d-ac5f-fffe8336d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[29 59 17 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ba61c-c064-49e7-9d36-74ade9ed6234",
   "metadata": {},
   "source": [
    "# Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2f0bc-e2d5-4962-81d4-65c08fbed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 60\n",
    "df_mapped[df_mapped['cosine_cluster_name'] == cluster].sort_values(by='cosine_distance_to_center_name', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f18397-a9d0-410e-ab91-8fc330f9251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cluster\n",
    "df_mapped_cleaned[df_mapped_cleaned['cosine_cluster_name'] == cluster].sort_values(by='cosine_distance_to_center_name', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
