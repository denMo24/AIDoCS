{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c85bf8-5d98-40e8-9d84-03d001dfb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "\n",
    "\n",
    "def create_all_results_json():\n",
    "    directory = os.getcwd()  # Get the current working directory\n",
    "    output_file = os.path.join(directory, 'all_results.json')\n",
    "\n",
    "    def read_json_files(directory):\n",
    "        results = []\n",
    "        seen_ids = set()  # To keep track of unique entries based on '_id'\n",
    "\n",
    "        # Loop through all files in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.startswith(\"r_\") and filename.endswith(\".json\"):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        if isinstance(data, dict):  # In case the data is a single dictionary\n",
    "                            data = [data]\n",
    "                        # Ensure unique entries based on '_id'\n",
    "                        for entry in data:\n",
    "                            if isinstance(entry, dict):  # Ensure each entry is a dictionary\n",
    "                                _id = entry.get('_id')\n",
    "                                if _id and _id not in seen_ids:\n",
    "                                    seen_ids.add(_id)\n",
    "                                    results.append(entry)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error decoding JSON from file: {filepath}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_to_json(data, output_file):\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Read the JSON files and combine the results\n",
    "    combined_results = read_json_files(directory)\n",
    "\n",
    "    # Save the combined results to a single JSON file\n",
    "    save_to_json(combined_results, output_file)\n",
    "\n",
    "    print(f\"Combined results saved to {output_file}\")\n",
    "\n",
    "# Usage example to create all_results.json\n",
    "create_all_results_json()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8074273-ff99-4e23-93d0-e90203f7ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71611f1-3956-44ac-b848-1aa8505c3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_all_results_json():\n",
    "    directory = os.getcwd()  # Get the current working directory\n",
    "    file_path = os.path.join(directory, 'all_results.json')\n",
    "    input_json = None\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists. Proceeding to load.\")\n",
    "        # Load the JSON file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                input_json = json.load(file)\n",
    "            print(\"File loaded successfully.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON file.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return input_json\n",
    "\n",
    "# Usage example to load all_results.json\n",
    "input_json = load_all_results_json()\n",
    "\n",
    "# Optionally print the loaded JSON content to verify\n",
    "# print(json.dumps(input_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afc522-3d62-4d61-9f0e-28dfea93976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data_mapping = {\n",
    "    \"chiefcomplaint\":      {\"collection\": \"ED-Triage-prammed\", \"field\": \"chiefcomplaint\"},\n",
    "    \"medication_reported\": {\"collection\": \"ED-Medrecon-prammed\", \"field\": \"name\"},\n",
    "    \"medication_pyxis\":    {\"collection\": \"ED-Pyxis-prammed\", \"field\": \"name\"},\n",
    "    \"diagnosis\":           {\"collection\": \"ED-Diagnosis-prammed\", \"field\": \"icd_title\"},\n",
    "    \"bp\":                  {\"collection\": [\"ED-Triage-prammed\", \"ED-VitalSigns-prammed\"], \"fields\": [(\"sbp\", \"dbp\")], \"format\": \"triage: {0} {1}; vitals: {2} {3}\"},\n",
    "    \"heartrate\":           {\"collection\": [\"ED-Triage-prammed\", \"ED-VitalSigns-prammed\"], \"fields\": [\"heartrate\"], \"format\": \"triage: {0}; vitals: {1}\"},\n",
    "    \"o2sat_triage\":        {\"collection\": [\"ED-Triage-prammed\", \"ED-VitalSigns-prammed\"], \"fields\": [\"o2sat\"], \"format\": \"triage: {0}; vitals: {1}\"},\n",
    "    \"resprate\":            {\"collection\": [\"ED-Triage-prammed\", \"ED-VitalSigns-prammed\"], \"fields\": [\"resprate\"], \"format\": \"triage: {0}; vitals: {1}\"},\n",
    "    \"temperature\":         {\"collection\": [\"ED-Triage-prammed\", \"ED-VitalSigns-prammed\"], \"fields\": [\"temperature\"], \"format\": \"triage: {0}; vitals: {1}\"},\n",
    "    \"pain_triage\":         {\"collection\": [\"ED-Triage-prammed\"], \"fields\": [\"pain\"], \"format\": \"triage: {0}\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5c538-50ee-4556-bef1-4434135d18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other questions of interest\n",
    "nominal_questions = [\n",
    "    \"chiefcomplaint\",\n",
    "    \"medication_reported\",\n",
    "    \"medication_pyxis\",\n",
    "    \"diagnosis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c38512-13e5-44ee-90c0-f9366d89e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_basic_dataframe(input_json):\n",
    "    rows = []\n",
    "\n",
    "    for i, record in enumerate(input_json):\n",
    "        record_id = record.get('_id', '')\n",
    "        \n",
    "        for question, answer in record['answers'].items():\n",
    "            rows.append([i, record_id, question, answer, ''])\n",
    "    \n",
    "    df_basic = pd.DataFrame(rows, columns=['json_i', '_id', 'question', 'answer', 'data'])\n",
    "    return df_basic\n",
    "\n",
    "# Usage example\n",
    "\n",
    "df_basic = create_basic_dataframe(input_json)\n",
    "df_basic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8a3d2-de02-4553-bfc1-44f6bd1b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ee9ea-8ec3-4141-81e3-be39d7bceda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b28f4-f868-4101-bf8d-8270c1cf977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(question, json_data_used, question_data_mapping):\n",
    "    data_info = question_data_mapping.get(question)\n",
    "    if not data_info:\n",
    "        return None\n",
    "\n",
    "    if isinstance(data_info[\"collection\"], list):\n",
    "        values = []\n",
    "        for field_pair in data_info[\"fields\"]:\n",
    "            if isinstance(field_pair, tuple):\n",
    "                triage_values = [json_data_used.get(\"ED-Triage-prammed\", {}).get(field, \"nan\") for field in field_pair]\n",
    "                vitals_values = [json_data_used.get(\"ED-VitalSigns-prammed\", {}).get(field, \"nan\") for field in field_pair]\n",
    "                values.extend(triage_values + vitals_values)\n",
    "            else:\n",
    "                triage_value = json_data_used.get(\"ED-Triage-prammed\", {}).get(field_pair, \"nan\")\n",
    "                vitals_value = json_data_used.get(\"ED-VitalSigns-prammed\", {}).get(field_pair, \"nan\")\n",
    "                values.extend([triage_value, vitals_value])\n",
    "        return data_info[\"format\"].format(*values)\n",
    "    else:\n",
    "        collection_data = json_data_used.get(data_info[\"collection\"], {})\n",
    "        if collection_data is None:\n",
    "            collection_data = {}\n",
    "        return collection_data.get(data_info[\"field\"], \"nan\")\n",
    "\n",
    "def fill_data_column(df, input_json, question_data_mapping):\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        json_i = row['json_i']\n",
    "        record = input_json[json_i]\n",
    "        \n",
    "        data = format_data(question, record['json_data_used'], question_data_mapping)\n",
    "        df.at[index, 'data'] = data if data is not None else 'nan'\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221df2bf-2996-4347-a2bd-82f41d1a2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = fill_data_column(df_basic, input_json, question_data_mapping)\n",
    "df_filled.insert(df_filled.columns.get_loc('data') + 1, 'Class', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f30515-3499-405c-bf59-23e62682ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d111c-319b-4c83-aa16-9f1b337e221c",
   "metadata": {},
   "source": [
    "# Postprocessing with openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfbc20-82cf-44fe-a3f4-e51bf207cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unprocessed = df_filled.copy()\n",
    "df_unprocessed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d53ab6-55e4-4ba4-a5b1-db34c3f5b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Openai aidocs kernel brauchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4575cd4-e666-4453-a215-2417047e5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde9c87-beae-49f1-b8be-7d6b64a8a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a specified .env file for secure API key storage\n",
    "env_path = '/home/msd4/aidocsMosi/openAI_Token.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"No API key found. Please set your OPENAI_API_KEY in the .env file.\")\n",
    "\n",
    "# Initialize the OpenAI client with the retrieved API key\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee43f0-026d-4de7-87b9-8a83ab582dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final\n",
    "prompt_template_conditions_data = \"\"\"\n",
    "Please extract the relevant medical conditions from the following text and return a comma-separated list. Follow these rules:\n",
    "- Convert all items to lower case.\n",
    "- Write out abbreviations.\n",
    "- Remove duplicates, keeping only one instance.\n",
    "- If the text contains a specific subgroup (e.g., \"Coffee Ground Emesis\"), keep the original and also add the general terminology.\n",
    "- Expand terms with multiple components into all relevant variations without losing medical context. For example:\n",
    "  - \"s/p Fall, L Hand injury, Head injury\" should become \"s/p fall, status post fall, l hand injury, left hand injury, hand injury, head injury\".\n",
    "  - \"TETANUS TOXOID INOCULAT\" should become \"tetanus toxoid inoculation, tetanus inoculation, tetanus\".\n",
    "  - \"Anemia, unspecified\" should become \"anemia unspecified, anemia\".\n",
    "  - \"L Knee swelling, Abnormal labs\" should become \"l knee swelling, left knee swelling, knee swelling, abnormal labs, abnormal lab results\".\n",
    "  - \"Long term (current) use of aspirin\" should become \"long term (current) use of aspirin, long term use of aspirin, use of aspirin, aspirin use, aspirin\".\n",
    "  - \"Lower back pain, MVC, Neck pain\" should become \"lower back pain, motor vehicle accident, mvc, neck pain\".\n",
    "  - \"Post-traumatic stress disorder, unspecified\" should become \"post-traumatic stress disorder, post traumatic stress disorder, traumatic stress disorder\".\n",
    "\n",
    "Here is the text:\n",
    "'''\n",
    "{text}\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac0378-e3b3-4d15-82f1-c4272c1ed914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added \"Do not list diagnoses or conditions that that are ruled out\", because rag-llm tended to fill out max length with \"ruled out\"-conditions in some cases (2 seen)\n",
    "\n",
    "prompt_template_conditions_answer = \"\"\"\n",
    "Please extract the relevant medical conditions, diagnosis and issues from the following text and return a comma-separated list. Follow these rules:\n",
    "- Convert all items to lower case.\n",
    "- Write out abbreviations where necessary.\n",
    "- Remove single numbers and special characters.\n",
    "- Do not use line breaks; keep everything in one line.\n",
    "- Condense longer descriptions into single words or short phrases while retaining medical meaning.\n",
    "- Correct any obvious spelling mistakes (e.g., \"hest pain\" to \"chest pain\").\n",
    "- Keep general terms to ensure medical context but reduce unnecessary variations. For example:\n",
    "  - \"No previous health problems mentioned.\\n- No persisting health problems mentioned.\\n- Symptoms reported: severe pain (rated 7 on a scale of 0 to 10), started a few hours ago, getting worse.\" should become \"no previous health problems, severe pain\".\n",
    "  - \"Chest pain\\n- Low blood pressure\\n- Tachypnea (respiratory rate of 19 breaths per minute, which is higher than normal)\\n- Bradycardia (heart rate of 67 beats per minute, which is lower than normal)\\n- Possible hypotension (blood pressure of 121 over 80)\" should become \"chest pain, low blood pressure, tachypnea, bradycardia, possible hypotension\".\n",
    "  - \"Unsteadiness on feet\" should become \"unsteadiness on feet, unsteadiness\".\n",
    "- Also list the wounds and injuries.\n",
    "- List the content in parentheses also separately.\n",
    "- Do not list diagnoses or conditions that that are ruled out.\n",
    "Here is the text:\n",
    "'''\n",
    "{text}\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05479a6-212c-4a26-8b9d-b592d195bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_drugs_data = \"\"\"\n",
    "Please extract the relevant drugs mentioned in the following text and return a comma-separated list in CSV format. Follow these rules:\n",
    "- Clean all special characters, including dots.\n",
    "- Convert all terms to lowercase.\n",
    "- Ensure forms and dosages are not mentioned alone without the drug name or active ingredient.\n",
    "- Keep the translated original term and append variations. Variations should include:\n",
    "  - One original term with dosage.\n",
    "  - One without dosage.\n",
    "  - Only market label name if present in data but keep original label names.\n",
    "  - Include the active ingredient name if present.\n",
    "- For example:\n",
    "  - \"Lantus Solostar U-100 Insulin\" should become \"lantus solostar u-100 insulin, lantus solostar insulin, lantus solostar, lantus insulin, insulin\".\n",
    "  - \"Morphin 2 tabletten\" should become \"morphine 2 tabs, morphine tabs, morphine\".\n",
    "  - \"DiphenhydrAMINE 25mg CAP\" should become \"diphenhydramine 25mg cap, diphenhydramine 25mg capsules, diphenhydramine capsules, diphenhydramine 25mg, diphenhydramine\".\n",
    "  - \"Oxycodone [OxyContin]\" should become \"oxycodone oxycontin, oxycodone, oxycontin\".\n",
    "  - \"Vitamin D2\" should become \"vitamin d2, vit d2\".\n",
    "  - \"MetRONIDAZOLE (Flagyl)\" should become \"metronidazole flagyl, metronidazole, flagyl\".\n",
    "  - \"Tetanus-DiphTox-Acellular Pertuss\" should become \"tetanus-diphtox-acellular pertuss, tetanus diphtox acellular pertuss, tetanus diphteria pertussis vaccine, tetanus diphteria pertussis, tetanus vaccine, diphteria vaccine, pertussis vaccine\".\n",
    "  - \"Ampicillin-Su 3g/100mL 100mL BAG\" should become \"ampicillin-su 3g/100ml 100ml bag, ampicillin-su 3g/100ml 100ml, ampicillin-su bag, ampicillin-su, ampicillin\".\n",
    "- Remove hyphens after keeping the original term.\n",
    "- Remove duplicates from the final CSV list.\n",
    "- Ensure the final list is comma-separated.\n",
    "\n",
    "Here is the text:\n",
    "'''\n",
    "{text}\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050daa0-0b62-4568-92c8-458131985c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final\n",
    "prompt_template_drugs_answer = \"\"\"\n",
    "Please extract the relevant drugs mentioned in the following text and return a comma-separated list in CSV format. Follow these rules:\n",
    "- Clean all special characters, including dots.\n",
    "- Convert all terms to lowercase.\n",
    "- Ensure forms and dosages are not mentioned alone without the drug name or active ingredient.\n",
    "- If an item has only dosage or form information (e.g., \"2 tabs in the morning\"), mark it as \"2 tabs with no further information\".\n",
    "- Keep the translated original term and append variations within the same item. Variations should include:\n",
    "  - One original term with dosage.\n",
    "  - One without dosage.\n",
    "  - Only market label name if present in data but keep original label names.\n",
    "  - Include the active ingredient name if present.\n",
    "- Variations should only be done within the same text-list-item and not combined with other items.\n",
    "- Remove hyphens after keeping the original term.\n",
    "- Remove duplicates from the final CSV list.\n",
    "- Remove non-drug related information.\n",
    "- If the text contains valid drug names without additional information, include them as they are.\n",
    "- If no drug-related information is available, insert only: \"no information at all\".\n",
    "- Ensure the final list is comma-separated.\n",
    "\n",
    "Here is the text:\n",
    "'''\n",
    "{text}\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cde0fe-1463-4aeb-97db-54e932fb5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(text, template):\n",
    "    prompt = template.format(text=text)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def gpt_make_csv_array(text, template):\n",
    "    response_text = call_openai(text, template)\n",
    "    csv_array = [item.strip() for item in response_text.split(',')]\n",
    "    print(response_text)\n",
    "    return csv_array\n",
    "\n",
    "def loop_gpt_make_csv(df, df_column_name, template, questions_to_process, start=0, end=None):\n",
    "    df_input = df.copy()\n",
    "    cell_of_column_to_process = df_column_name\n",
    "    new_column_name = df_column_name + '_gpt_csv'\n",
    "\n",
    "    if end is None:\n",
    "        end = len(df_input)\n",
    "        \n",
    "    # Filter the dataframe based on the specified questions\n",
    "    df_to_loop_through = df_input[df_input['question'].isin(questions_to_process)].iloc[start:end]\n",
    "\n",
    "    # Initialize the new column with empty lists\n",
    "    df_input[new_column_name] = [[] for _ in range(len(df_input))]\n",
    "\n",
    "    for idx, row in df_to_loop_through.iterrows():\n",
    "        text = str(row[cell_of_column_to_process])\n",
    "        csv_array = gpt_make_csv_array(text, template)\n",
    "        df_input.at[idx, new_column_name] = csv_array  # Assign the array directly\n",
    "\n",
    "    return df_input\n",
    "\n",
    "\n",
    "def loop_gpt_make_csv(df, df_column_name, template, questions_to_process, start=0, end=None):\n",
    "    df_input = df.copy()\n",
    "    cell_of_column_to_process = df_column_name\n",
    "    new_column_name = 'data_gpt' if df_column_name == 'data' else 'answer_gpt'\n",
    "\n",
    "    if new_column_name not in df_input.columns:\n",
    "        # Initialize the new column with empty lists if it doesn't exist\n",
    "        df_input[new_column_name] = [[] for _ in range(len(df_input))]\n",
    "\n",
    "    if end is None:\n",
    "        end = len(df_input)\n",
    "        \n",
    "    # Filter the dataframe based on the specified questions\n",
    "    df_to_loop_through = df_input[df_input['question'].isin(questions_to_process)].iloc[start:end]\n",
    "\n",
    "    for idx, row in df_to_loop_through.iterrows():\n",
    "        if not row[new_column_name]:  # Only fill if the cell is empty\n",
    "            text = str(row[cell_of_column_to_process])\n",
    "            csv_array = gpt_make_csv_array(text, template)\n",
    "            df_input.at[idx, new_column_name] = csv_array  # Assign the array directly\n",
    "\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a998fb-fc70-4108-aa88-cc0733ffdbf5",
   "metadata": {},
   "source": [
    "### Postprocess conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add59242-499d-491c-abfc-0ce2713be146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess data\n",
    "processed_df = df_unprocessed\n",
    "questions_to_process = ['chiefcomplaint', 'diagnosis']\n",
    "processed_df = loop_gpt_make_csv(processed_df, 'data', prompt_template_conditions_data, questions_to_process, start=0, end=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47d48da8-b29e-4372-8706-5c5325051531",
   "metadata": {},
   "source": [
    "filtered_df = processed_df.loc[processed_df['question'].isin(['chiefcomplaint', 'diagnosis'])].copy()\n",
    "filtered_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668ae61-9638-42c6-977d-3c30b3fab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV or handle the DataFrame as needed\n",
    "processed_df.to_csv('processed_output-conditions-data-gpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b56b75-4203-49e7-b96d-5cc44f728724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets progress with postprossessing\n",
    "#df_in = processed_df\n",
    "questions_to_process = ['chiefcomplaint', 'diagnosis']\n",
    "processed_df = loop_gpt_make_csv(processed_df, 'answer', prompt_template_conditions_answer, questions_to_process, start=0, end=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df88bf-bd45-4cd6-8d0b-c69ff0d763af",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7be1b0db-889c-41d8-b2d2-b286c7bdac18",
   "metadata": {},
   "source": [
    "filtered_df = processed_df.loc[processed_df['question'].isin(['chiefcomplaint', 'diagnosis'])].copy()\n",
    "filtered_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6a13c-b051-4e48-b8e8-ee81c2a9067d",
   "metadata": {},
   "source": [
    "### postprocess medication"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4d59cb7-bf7d-4665-929c-4d5a39ce1bee",
   "metadata": {},
   "source": [
    "# Filter the DataFrame\n",
    "inspect_df_medi = processed_df[processed_df['question'].isin(['medication_reported', 'medication_pyxis'])].copy()\n",
    "inspect_df_medi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203915c-d445-41b3-9757-7c5dd0896dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_process = ['medication_reported', 'medication_pyxis']\n",
    "processed_df = loop_gpt_make_csv(processed_df, 'data', prompt_template_drugs_data, questions_to_process, start=0, end=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f24a92f3-7c4b-4088-8c91-021e4b34b26c",
   "metadata": {},
   "source": [
    "filtered_df = processed_df_medication_data.loc[processed_df_medication_data['question'].isin(['medication_reported', 'medication_pyxis'])].copy()\n",
    "filtered_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae811d1-59da-4416-848f-dc3e8743cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_process = ['medication_reported', 'medication_pyxis']\n",
    "processed_df = loop_gpt_make_csv(processed_df, 'answer', prompt_template_drugs_answer, questions_to_process, start=0, end=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a2ad4-615a-49a3-b87f-8406e43f6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc04312f-108f-47c5-a938-0cdf4cccdfac",
   "metadata": {},
   "source": [
    "filtered_df = processed_df_medication_answer.loc[processed_df_medication_answer['question'].isin(['medication_reported', 'medication_pyxis'])].copy()\n",
    "filtered_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cffc4-78b6-4a78-b967-194ee5054deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV or handle the DataFrame as needed\n",
    "processed_df.to_csv('processed_df-completeGpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a925b9-13b5-42e5-807b-ae0260835589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81370c2b-2386-4c92-8ea5-9f096b7f03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE isnull, to see if postprocessing with gpt did work\n",
    "\n",
    "\n",
    "# Check for empty cells in the DataFrame\n",
    "empty_cells = processed_df.isnull() | (processed_df.applymap(lambda x: len(x) == 0 if isinstance(x, list) else False))\n",
    "\n",
    "# Get rows with any empty cells\n",
    "rows_with_empty_cells = processed_df[empty_cells.any(axis=1)]\n",
    "\n",
    "# Get unique questions from those rows\n",
    "unique_questions_with_empty_cells = rows_with_empty_cells['question'].unique()\n",
    "\n",
    "# Print the unique questions\n",
    "print('Those question where not affected by gpt postprocessing ')\n",
    "print(unique_questions_with_empty_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5079910-2d48-40dd-a99c-e4b2be133cb4",
   "metadata": {},
   "source": [
    "## Classification of nominals by cosine_similarity between gpt-postprocessed data and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e8906-1084-45ed-9e45-d677b2a2bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_done = processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1065d26-c53d-44fb-a222-7f1b6a61cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the BlueBERT tokenizer and model for embeddings\n",
    "model_name = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee78f20-a552-43f5-8240-06ee6738f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def classify_nominals(df):\n",
    "    classifications = []\n",
    "    all_similarities = []\n",
    "    cutoff_positive = 0.85\n",
    "    cutoff_negative = 0.6  # Average cutoff for FN\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        data_gpt_csv_embeddings = [compute_embeddings(item) for item in row['data_gpt']]\n",
    "        answer_gpt_csv_embeddings = [compute_embeddings(item) for item in row['answer_gpt']]\n",
    "        \n",
    "        row_classification = \"FP\"\n",
    "        row_similarities = []\n",
    "        for data_embedding in data_gpt_csv_embeddings:\n",
    "            max_similarity = 0\n",
    "            for answer_embedding in answer_gpt_csv_embeddings:\n",
    "                similarity = cosine_similarity([data_embedding], [answer_embedding])[0][0]\n",
    "                row_similarities.append(similarity)\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                if similarity >= cutoff_positive:\n",
    "                    row_classification = \"TP\"\n",
    "                    break\n",
    "            if row_classification == \"TP\":\n",
    "                break\n",
    "        \n",
    "        # Calculate the average similarity score\n",
    "        average_similarity = sum(row_similarities) / len(row_similarities) if row_similarities else 0\n",
    "        if row_classification == \"FP\" and average_similarity <= cutoff_negative:\n",
    "            row_classification = \"FN\"\n",
    "        \n",
    "        classifications.append(row_classification)\n",
    "        all_similarities.append(row_similarities)\n",
    "    \n",
    "    df['Class'] = classifications\n",
    "    df['Similarities'] = all_similarities\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44896568-d6cb-4e72-9b01-f7c3b2806f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(df, question_types):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame for specified question types and computes classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the classification results.\n",
    "    question_types (list): List of question types to filter the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing question types, TP count, FP count, FN count, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to only include rows with the specified question types\n",
    "    filtered_df = df[df['question'].isin(question_types)]\n",
    "\n",
    "    # Calculate counts for TP, FP, FN\n",
    "    tp_count = (filtered_df['Class'] == 'TP').sum()\n",
    "    fp_count = (filtered_df['Class'] == 'FP').sum()\n",
    "    fn_count = (filtered_df['Class'] == 'FN').sum()\n",
    "\n",
    "    # Calculate Precision, Recall, and F1-score\n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Round the precision, recall, and F1-score to 2 decimal places\n",
    "    precision = round(precision, 2)\n",
    "    recall = round(recall, 2)\n",
    "    f1_score = round(f1_score, 2)\n",
    "\n",
    "    # Create a dictionary with the results\n",
    "    metrics = {\n",
    "        'Question_types': question_types,\n",
    "        'TP_count': tp_count,\n",
    "        'FP_count': fp_count,\n",
    "        'FN_count': fn_count,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_score': f1_score\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5e475-e02f-42f0-ac2a-1b362051c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY METHODS\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort_to_lastscore(df, cls='TP', order='asc', questions=None):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame for a specific class and questions, and sort by the last score.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    cls (str): The class to filter by.\n",
    "    order (str): The sort order ('asc' for ascending, 'desc' for descending).\n",
    "    questions (list or str): The questions to filter by ('all' for all relevant questions).\n",
    "\n",
    "    like:\n",
    "    sorted_df = sort_to_lastscore(df, cls='TP', order='asc', questions=['chiefcomplaint', 'medication_reported'])\n",
    "    sorted_df = sort_to_lastscore(df, cls='TP', order='asc', questions='all')\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The sorted copy of the DataFrame.\n",
    "    \"\"\"\n",
    "    if questions == 'all':\n",
    "        questions = ['chiefcomplaint', 'medication_reported', 'medication_pyxis', 'diagnosis']\n",
    "    elif isinstance(questions, str):\n",
    "        questions = [questions]\n",
    "\n",
    "    # Filter the DataFrame for the specified class and questions\n",
    "    filtered_df = df[(df['Class'].str.upper() == cls.upper()) & (df['question'].isin(questions))]\n",
    "\n",
    "    # Sort the DataFrame by 'last_score'\n",
    "    sorted_df = filtered_df.copy().sort_values(by='last_score', ascending=(order == 'asc'))\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_last_score_to_all_nominals(df):\n",
    "    \"\"\"\n",
    "    Extract the last score from the 'Similarities' column for every row and insert it into a new column 'last_score'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an additional 'last_score' column.\n",
    "    \"\"\"\n",
    "    def extract_last_score(similarities):\n",
    "        try:\n",
    "            # Convert the string representation to an actual list\n",
    "            similarities_list = ast.literal_eval(similarities)\n",
    "            # Return the last item of the list\n",
    "            return similarities_list[-1] if similarities_list else float('nan')\n",
    "        except (ValueError, SyntaxError):\n",
    "            return float('nan')\n",
    "    \n",
    "    # Apply the function to extract the last score\n",
    "    df['last_score'] = df['Similarities'].apply(extract_last_score)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_last_score_to_all_nominals(df):\n",
    "    \"\"\"\n",
    "    Extract the last score from the 'Similarities' column for every row and insert it into a new column 'last_score'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an additional 'last_score' column.\n",
    "    \"\"\"\n",
    "    def extract_last_score(similarities):\n",
    "        try:\n",
    "            # Convert the string representation to an actual list\n",
    "            similarities_list = ast.literal_eval(similarities)\n",
    "            # Return the last item of the list\n",
    "            return similarities_list[-1] if similarities_list else float('nan')\n",
    "        except (ValueError, SyntaxError):\n",
    "            return float('nan')\n",
    "    \n",
    "    # Apply the function to extract the last score\n",
    "    df['last_score'] = df['Similarities'].apply(extract_last_score)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_last_score_to_all_nominals(df):\n",
    "    \"\"\"\n",
    "    Extract the last score from the 'Similarities' column for every row and insert it into a new column 'last_score'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with an additional 'last_score' column.\n",
    "    \"\"\"\n",
    "    def extract_last_score(similarities):\n",
    "        if isinstance(similarities, list):\n",
    "            return similarities[-1] if similarities else float('nan')\n",
    "        return float('nan')\n",
    "    \n",
    "    # Apply the function to extract the last score\n",
    "    df['last_score'] = df['Similarities'].apply(extract_last_score)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_scores(df, questions=None, cutoff=0.85):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of classification scores with a cutoff line.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    questions (list or str): The questions to filter by ('all' for all relevant questions).\n",
    "    cutoff (float): The cutoff line for the scores.\n",
    "    \"\"\"\n",
    "    if questions == 'all':\n",
    "        questions = ['chiefcomplaint', 'medication_reported', 'medication_pyxis', 'diagnosis']\n",
    "    elif isinstance(questions, str):\n",
    "        questions = [questions]\n",
    "\n",
    "    # Filter the DataFrame for the specified questions\n",
    "    filtered_df = df[df['question'].isin(questions)].copy()\n",
    "\n",
    "    # Define markers and colors for each question and class\n",
    "    question_markers = {\n",
    "        'chiefcomplaint': 'o',\n",
    "        'medication_reported': 's',\n",
    "        'medication_pyxis': '^',\n",
    "        'diagnosis': 'D'\n",
    "    }\n",
    "\n",
    "    class_colors = {\n",
    "        'TP': 'green',\n",
    "        'FP': 'darkorange',\n",
    "        'FN': 'red'\n",
    "    }\n",
    "\n",
    "    # Plot the scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for question in questions:\n",
    "        question_df = filtered_df[filtered_df['question'] == question]\n",
    "        plt.scatter(\n",
    "            question_df['json_i'], \n",
    "            question_df['last_score'], \n",
    "            label=question, \n",
    "            marker=question_markers.get(question, 'o'), \n",
    "            c=question_df['Class'].map(class_colors),\n",
    "            edgecolor='k',\n",
    "            s=100\n",
    "        )\n",
    "    \n",
    "    # Plot the cutoff line\n",
    "    plt.axhline(y=cutoff, color='blue', linestyle='--', label=f'Cutoff = {cutoff}')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Last Score')\n",
    "    plt.title('Classification Scores with Cutoff Line')\n",
    "    plt.legend(title='Questions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_csv_to_df(file_name):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_name (str): The name of the CSV file to load.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(current_dir, file_name)\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "def find_specific_rows_with_only_one_arrayitem(df, column_name, target_array):\n",
    "    \"\"\"\n",
    "    Find rows where the specified column contains exactly the target array.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to search.\n",
    "    column_name (str): The column to search within.\n",
    "    target_array (list): The array to match.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with matching rows.\n",
    "    \"\"\"\n",
    "    # Define a function to check if the column value matches the target array\n",
    "    def matches_target(value):\n",
    "        try:\n",
    "            # Convert the string representation to an actual list\n",
    "            value_list = ast.literal_eval(value)\n",
    "            return value_list == target_array\n",
    "        except (ValueError, SyntaxError):\n",
    "            return False\n",
    "    \n",
    "    # Find rows where the column matches the target array\n",
    "    matching_rows = df[df[column_name].apply(matches_target)]\n",
    "    \n",
    "    return matching_rows\n",
    "\n",
    "# Example usage if df is to be loaded from a csv that contains allready gpt preprocessing for nominals, if not done here\n",
    "#classified_df = load_csv_to_df('df_nominal_classified-de.csv')\n",
    "#classified_df = load_csv_to_df('df_nominal_classified-de.csv')\n",
    "#classified_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ce08c-66a4-4aef-97f1-9edf02d417c4",
   "metadata": {},
   "source": [
    "#### classification with embeddings ______ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ef9c4-d372-4f6a-895a-11c60090d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification into TP, FP, FN with COS.\n",
    "\n",
    "\n",
    "classified_df = classify_nominals(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4cc09e-1f3e-4cb1-a544-2df3b0b351c7",
   "metadata": {},
   "source": [
    "#### Check for FN ['no information at all']______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6cbb9-e9a8-487c-8510-5adc1b8f28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows = find_specific_rows_with_only_one_arrayitem(classified_df, 'answer_gpt', ['no information at all'])\n",
    "matching_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f38a3d-de87-4930-a48a-09ad494c895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0bdbc-f182-49cc-8dc8-456614f80c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nach inspektion mussten unter answer_gpt alle mit 'no information at all' von hand auf FN gesetzt werden, da kein cutoff gesetzt werden kann der klar ist. war bei en-dialogen nicht aufgefallen da sehr wenige.\n",
    "df_copysave = classified_df.copy()\n",
    "classified_df.loc[classified_df['answer_gpt'].apply(lambda x: ast.literal_eval(x) == ['no information at all']), 'Class'] = 'FN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d291ca-fd78-4480-952a-60f38b5addca",
   "metadata": {},
   "source": [
    "#### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb5e6-77f0-43a5-96a5-56394f742ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_as_json(metrics, question_types):\n",
    "    # Function to convert numpy data types to Python native types for JSON serialization\n",
    "    def convert(o):\n",
    "        if isinstance(o, np.int64): return int(o)  \n",
    "        raise TypeError\n",
    "    \n",
    "    # Generate the file name based on the given question types\n",
    "    file_name = \"metrics_nominal-\" + \"_\".join(question_types) + \".json\"\n",
    "    \n",
    "    # Convert the metrics dictionary to a JSON string, using the convert function for non-serializable types\n",
    "    metrics_json = json.dumps(metrics, indent=4, default=convert)\n",
    "    \n",
    "    # Write the JSON string to a file with the dynamically generated name\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(metrics_json)\n",
    "    \n",
    "    print(f\"Metrics saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e042b-484e-4892-9db2-c52325f4f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['chiefcomplaint', 'diagnosis', 'medication_reported', 'medication_pyxis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7637094-2954-47d6-b50e-a837ea13dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['chiefcomplaint', 'diagnosis', 'medication_reported', 'medication_pyxis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c95d13-b97b-40a0-b2e6-9167489c7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['chiefcomplaint', 'diagnosis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c39983-fed9-4a1a-928e-b3176fdfabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['medication_reported', 'medication_pyxis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e2184-ae25-4b2f-bf98-e7a78cc395bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['diagnosis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4783d-8682-4ad2-ae15-2c60648ef17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['chiefcomplaint']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac804ba2-87ff-4acb-9bc2-5c8a42b3470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['medication_reported']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006451e2-6692-472d-ab84-412d36a0ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the question types to filter\n",
    "question_types = ['medication_pyxis']\n",
    "# Compute the classification metrics\n",
    "metrics = compute_classification_metrics(classified_df, question_types)\n",
    "save_metrics_as_json(metrics, question_types)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd843863-a91c-4e7a-8b3c-4a04be076326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV or handle the DataFrame as needed\n",
    "classified_df.to_csv('df_nominal_classified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb41d10-1207-422a-ad15-6b0382953fc9",
   "metadata": {},
   "source": [
    "# Inspection\n",
    "\n",
    "To visually inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fc5a1-3fa5-4231-b6ec-5765c037c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d45e0b-25b7-4003-b786-e1338cedf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Class and Scores to see if the Set cutoff is ok\n",
    "classified_df_lastscored= add_last_score_to_all_nominals(classified_df)\n",
    "classified_df_lastscored.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d7849-4a67-4ab8-adb3-8edb73f83b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_scores(classified_df_lastscored, questions='all')\n",
    "# Save the plot as a PNG file\n",
    "questions_str = '_'.join(['all'])\n",
    "filename = f'nominal_score_cutoff_{questions_str}.png'\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "print(f'Plot saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03106466-88b3-4c27-9051-839f70d499bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_scores(classified_df_lastscored, questions='chiefcomplaint')\n",
    "questions_str = '_'.join(['chiefcomplaint'])\n",
    "filename = f'nominal_score_cutoff_{questions_str}.png'\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "print(f'Plot saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70041388-7cb5-42c4-ae2f-a0f14925591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_scores(classified_df_lastscored, questions='medication_reported')\n",
    "questions_str = '_'.join(['medication_reported'])\n",
    "filename = f'nominal_score_cutoff_{questions_str}.png'\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "print(f'Plot saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af9716-3c74-4384-aaef-39031ec20cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_scores(classified_df_lastscored, questions='medication_pyxis')\n",
    "questions_str = '_'.join(['medication_pyxis'])\n",
    "filename = f'nominal_score_cutoff_{questions_str}.png'\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "print(f'Plot saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830b8c9-b60a-46b3-b6cd-1c9731807315",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_scores(classified_df_lastscored, questions='diagnosis')\n",
    "questions_str = '_'.join(['diagnosis'])\n",
    "filename = f'nominal_score_cutoff_{questions_str}.png'\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "print(f'Plot saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebba1b0-6c36-4210-b056-42ff561bf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect if the lower TP are still reasonable and confirming the set cutoff, or that cutoff must be changed.\n",
    "#sorted_df_tp = sort_to_lastscore(classified_df_lastscored, cls='TP', order='asc', questions=['chiefcomplaint', 'medication_reported'])\n",
    "#sorted_df_fp = sort_to_lastscore(classified_df_lastscored, cls='FP', order='desc', questions='all')\n",
    "#sorted_df_fp = sort_to_lastscore(classified_df_lastscored, cls='FP', order='desc', questions=['diagnosis'])\n",
    "\n",
    "\n",
    "sorted_df_fp_diagnosis = sort_to_lastscore(classified_df_lastscored, cls='FP', order='desc', questions=['diagnosis'])\n",
    "sorted_df_fp_diagnosis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fb1f8-360e-4315-834c-8691ce3e599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_tp_diagnosis = sort_to_lastscore(classified_df_lastscored, cls='TP', order='asc', questions=['diagnosis'])\n",
    "sorted_df_tp_diagnosis.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5482e8b-b01b-4def-b348-0984acb9e060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715952c1-b433-426f-a72b-b4018fd39074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96c4fbf-4968-4d19-ac62-39b4eb3c21ae",
   "metadata": {},
   "source": [
    "# Numerical-Question Evaluation (like bp: 162 / 80):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbea23-a261-44d5-b8f5-0f659a6e2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classified_df_numerical = classified_df.copy()\n",
    "classified_df_numerical =df_filled.copy()\n",
    "classified_df_numerical.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0e4f9-f012-41ff-afaf-209beb9c7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions of interest\n",
    "numerical_questions = [\n",
    "    \"bp\",\n",
    "    \"heartrate\",\n",
    "    \"o2sat_triage\",\n",
    "    \"resprate\",\n",
    "    \"temperature\",\n",
    "    \"pain_triage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e412b-68f3-4177-bfc2-e74816e1e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_numerical_metric(metrics, file_name='numerical_metrics.json'):\n",
    "    \"\"\"\n",
    "    Saves the provided metrics dictionary into a JSON file.\n",
    "\n",
    "    Args:\n",
    "    metrics (dict): The metrics dictionary containing tp, fp, fn, precision, recall, f1_score, instances.\n",
    "    file_name (str): The name of the file to save the JSON data. Default is 'numerical_metrics.json'.\n",
    "    \"\"\"\n",
    "    # Convert the metrics dictionary to a JSON string\n",
    "    metrics_json = json.dumps(metrics, indent=4)\n",
    "    \n",
    "    # Write the JSON string to a file with the specified or dynamically generated name\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(metrics_json)\n",
    "    \n",
    "    print(f\"Numerical metrics saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769cbe2-6019-440b-a78a-cacd73302b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "numerical_questions = [\n",
    "    \"bp\",\n",
    "    \"heartrate\",\n",
    "    \"o2sat_triage\",\n",
    "    \"resprate\",\n",
    "    \"temperature\",\n",
    "    \"pain_triage\"\n",
    "]\n",
    "\n",
    "def classify_numerical_data(df_numerical):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    instances = 0\n",
    "    classifications = []\n",
    "\n",
    "    def is_match(data_value, answer):\n",
    "        if data_value % 1 == 0:\n",
    "            pattern = r'\\b{}\\b'.format(int(data_value))\n",
    "            return re.search(pattern, answer) is not None\n",
    "        else:\n",
    "            patterns = [\n",
    "                r'\\b{}\\b'.format(int(data_value)),\n",
    "                r'\\b{}\\b'.format(round(data_value)),\n",
    "                r'\\b{}\\b'.format(round(data_value - 0.5)),\n",
    "                r'\\b{}\\b'.format(round(data_value + 0.5))\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, answer):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    for index, row in df_numerical.iterrows():\n",
    "        data_values = re.findall(r'\\d+\\.?\\d*', str(row['data']))\n",
    "        if not data_values:\n",
    "            continue\n",
    "        \n",
    "        data_values = list(map(float, data_values))\n",
    "        instances += len(data_values)\n",
    "        answer = str(row['answer'])\n",
    "        row_classifications = []\n",
    "\n",
    "        answer_values = re.findall(r'\\d+\\.?\\d*', answer)\n",
    "        if not answer_values:\n",
    "            fn += len(data_values)\n",
    "            row_classifications.extend([\"FN\"] * len(data_values))\n",
    "        else:\n",
    "            for data_value in data_values:\n",
    "                if is_match(data_value, answer):\n",
    "                    tp += 1\n",
    "                    row_classifications.append(\"TP\")\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    row_classifications.append(\"FP\")\n",
    "\n",
    "        classifications.append(row_classifications)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    df_numerical_classified = df_numerical.copy()\n",
    "    df_numerical_classified['Class'] = [\"; \".join(cls) for cls in classifications]\n",
    "\n",
    "    cols = df_numerical_classified.columns.tolist()\n",
    "    data_index = cols.index('data')\n",
    "    cols.insert(data_index + 1, cols.pop(cols.index('Class')))\n",
    "    df_numerical_classified = df_numerical_classified[cols]\n",
    "\n",
    "    metrics = {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'instances': instances\n",
    "    }\n",
    "\n",
    "    return metrics, df_numerical_classified\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df_numerical = df_filled[df_filled['question'].isin(numerical_questions)]\n",
    "metrics, df_numerical_classified = classify_numerical_data(df_numerical)\n",
    "save_numerical_metric(metrics)\n",
    "print(metrics)\n",
    "#df_numerical_classified.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1341228-6579-4b68-8761-faa43a8b752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one if df stems from first nominal classification\n",
    "\n",
    "#classified_df_numerical = classified_df_numerical[classified_df_numerical['question'].isin(numerical_questions)]\n",
    "#df_numerical_classified = classified_df_numerical\n",
    "#metrics, classified_df_numerical = classify_numerical_data(classified_df_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361b5bf-e2b6-4720-b74b-fc34c04c2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_classified.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c014911a-9661-4c33-900f-eb539efeeb8a",
   "metadata": {},
   "source": [
    "# Save to CSV or handle the DataFrame as needed\n",
    "classified_df_numerical.to_csv('classified_df_numerical-de.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ddd84-660b-4193-b7db-ff4efc52fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# Get current date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Create file name with date\n",
    "file_name = f\"classified_df_numerical-{current_date}-de.csv\"\n",
    "\n",
    "# Save dataframe to CSV\n",
    "classified_df_numerical.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4512e-813e-41c9-badb-6c02ea23c901",
   "metadata": {},
   "source": [
    "## Save Nominal Total as CSV for savekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447f7c1-de92-4d2b-a480-41faa9f6eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nominal_total = pd.concat([df_nominal_drugs_classified, df_nominal_conditions_classified], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72dd58-3b35-4290-99fd-c340eb949a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nominal_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a415f2-882c-47e0-ade5-0d127287b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Create file name with date\n",
    "file_name = f\"df_nominal_total-{current_date}.csv\"\n",
    "\n",
    "# Save dataframe to CSV\n",
    "df_nominal_total.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d012f3-8c01-417f-93e5-01aa3e145218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    tp = sum(df['Class'] == \"TP\")\n",
    "    fp = sum(df['Class'] == \"FP\")\n",
    "    fn = sum(df['Class'] == \"FN\")\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for the combined DataFrame\n",
    "metrics_nominal_total = calculate_metrics(df_nominal_total)\n",
    "\n",
    "# Print the metrics\n",
    "metrics_nominal_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
