{
 "cells": [
  {
   "cell_type": "raw",
   "id": "015ddb1f-5635-492d-bcd9-d79489015df3",
   "metadata": {},
   "source": [
    "CC-PRAM\n",
    "\n",
    "Used Kernel: \n",
    "- eval_semantics\n",
    "\n",
    "Used Conda:\n",
    "- conda activate kmeans_env\n",
    "\n",
    "Sources: \n",
    "- adwanced k-means https://towardsdatascience.com/advanced-k-means-controlling-groups-sizes-and-selecting-features-a998df7e6745\n",
    "- cosine similarity for kmeans using l2 normalized embeddings\n",
    "\n",
    "pip install protobuf==3.20.3\n",
    "pip install jax libclang tensorflow-io-gcs-filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c831076-15bc-4a93-b958-fb68d9651370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from k_means_constrained import KMeansConstrained\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771b991-e5b3-4999-9459-52f3b0df49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2818e-6ec0-4191-a5cd-13992be034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to get embedding for a single text\n",
    "def get_single_text_embedding(text, tokenizer, model, device):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    inputs = tokenizer(cleaned_text, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(axis=1).cpu().numpy()\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(embedding, fix_embedding):\n",
    "    return cosine_similarity([embedding], [fix_embedding])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a85e-9bfa-42d3-a7e9-92d4d837cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BlueBERT tokenizer and model\n",
    "model_name = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d9cde-a934-4059-8a17-2e7fdc75c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Adjust the connection string as necessary\n",
    "db = client['MIMIC-IV']  # Replace with your database name\n",
    "collection = db['ED-Diagnosis']  # Replace with your collection name\n",
    "\n",
    "# Fetch data from MongoDB\n",
    "data = list(collection.find({}, {'_id': 0}))  # Fetch all key-value pairs except '_id'\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704347bb-6fda-4444-88d0-c0a5b251cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "df_original = df.sample(n=1000, random_state=42)\n",
    "df_sample = df_original.drop_duplicates(subset='icd_title') # drop duplicates, since we don't want to cluster same word-embeddings, but the similar ones.\n",
    "sample_size = len(df_sample)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fce12-8dfd-4d03-bdb4-d4243d7b50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the sample\n",
    "df_sample = df_sample.copy()  # Create a proper copy to avoid SettingWithCopyWarning\n",
    "df_sample.loc[:, 'icd_title_embedding'] = df_sample['icd_title'].apply(get_single_text_embedding, args=(tokenizer, model, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "\n",
    "# Stack the embeddings into a 2D array for clustering\n",
    "embeddings = np.vstack(df_sample['icd_title_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87becb4-184c-4f95-a692-f2ebc90cf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction and plot explained variance\n",
    "pca_inspect = PCA(n_components=100)\n",
    "reduced_embeddings_inspect = pca_inspect.fit_transform(embeddings)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = np.cumsum(pca_inspect.explained_variance_ratio_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid()\n",
    "plt.savefig('pca_explained_variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4472da-b5dc-4eb3-8ef3-64f767fdfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of components based on explained variance\n",
    "n_components = 65  # Adjust this based on the explained variance plot\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "df_sample['reduced_embeddings'] = list(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b490df2-0cc2-4a5e-90b5-6706f725c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the PCA-reduced embeddings using L2 norm\n",
    "# L2 normalization scales each vector to have a unit norm (length of 1). This is essential because it ensures that\n",
    "# the distance metric (cosine similarity) focuses on the direction of the vectors rather than their magnitudes.\n",
    "# This step makes the Euclidean distance between normalized vectors equivalent to cosine similarity, which is \n",
    "# crucial for clustering methods like k-means that rely on distance measures.\n",
    "normalized_embeddings = normalize(reduced_embeddings, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4ff25-36c4-4269-8905-89c57c7d9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SEEEMS TO WORK\n",
    "df_sample.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07559897-3007-4129-803f-7809f23d67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_size/5)\n",
    "print(sample_size/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45e9f-284a-4187-b046-776b238287df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters and the constraints\n",
    "\n",
    "size_min = 4  \n",
    "size_max = 5 \n",
    "best_k = 99\n",
    "\n",
    "# Perform K-means constrained clustering\n",
    "kmeans_constrained = KMeansConstrained(\n",
    "    n_clusters=best_k,\n",
    "    size_min=size_min,\n",
    "    size_max=size_max,\n",
    "    init=\"k-means++\",\n",
    "    n_init=200,\n",
    "    max_iter=1000,\n",
    "    random_state=1984\n",
    ")\n",
    "cosine_clusters = kmeans_constrained.fit_predict(normalized_embeddings)\n",
    "\n",
    "df_sample['cosine_cluster_icd_title'] = cosine_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63cf8c-3d6a-43cd-997c-9e240c77eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine distance of each point to its respective cluster center and add the values for a distinct marker within cluster, which can be used in PRAM (Post Randomization)\n",
    "cosine_distances = np.zeros(normalized_embeddings.shape[0])\n",
    "for i in range(normalized_embeddings.shape[0]):\n",
    "    cluster_center = kmeans_constrained.cluster_centers_[cosine_clusters[i]]\n",
    "    cosine_similarity_value = np.dot(normalized_embeddings[i], cluster_center)\n",
    "    cosine_distances[i] = 1 - cosine_similarity_value\n",
    "\n",
    "df_sample['cosine_distance_to_center_icd_title'] = cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8793d1-8acc-49a0-a483-bd268f48d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d7320-6ba0-457e-82c0-effc6f4146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from df_sample\n",
    "cluster_mapping = df_sample.set_index('icd_title')['cosine_cluster_icd_title'].to_dict()\n",
    "distance_mapping = df_sample.set_index('icd_title')['cosine_distance_to_center_icd_title'].to_dict()\n",
    "\n",
    "# Create a new DataFrame df_mapped from df_original with the new columns\n",
    "df_mapped = df_original.copy()\n",
    "df_mapped['cosine_cluster_icd_title'] = df_mapped['icd_title'].map(cluster_mapping)\n",
    "df_mapped['cosine_distance_to_center_icd_title'] = df_mapped['icd_title'].map(distance_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65ae42-c06d-46f8-9810-882b5efdbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d318588-bc46-40fc-baae-61a10684c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean and standard deviation for each cluster\n",
    "cluster_stats = df_mapped.groupby('cosine_cluster_icd_title')['cosine_distance_to_center_icd_title'].agg(['mean', 'std'])\n",
    "\n",
    "# Define a function to identify high outliers\n",
    "def is_high_outlier(row):\n",
    "    cluster = row['cosine_cluster_icd_title']\n",
    "    distance = row['cosine_distance_to_center_icd_title']\n",
    "    mean = cluster_stats.loc[cluster, 'mean']\n",
    "    std = cluster_stats.loc[cluster, 'std']\n",
    "    return distance > (mean + 3 * std)\n",
    "\n",
    "# Check if a cluster is valid based on unique counts, since we aim to use PRAM with a 3x3 matrix at least.\n",
    "def is_valid_cluster(cluster, df, min_unique_icd_titles=3, min_unique_distances=3):\n",
    "    icd_titles_count = df[df['cosine_cluster_icd_title'] == cluster]['icd_title'].nunique()\n",
    "    distances_count = df[df['cosine_cluster_icd_title'] == cluster]['cosine_distance_to_center_icd_title'].nunique()\n",
    "    return icd_titles_count >= min_unique_icd_titles and distances_count >= min_unique_distances\n",
    "\n",
    "# Identify valid clusters\n",
    "valid_clusters = [cluster for cluster in df_mapped['cosine_cluster_icd_title'].unique() if is_valid_cluster(cluster, df_mapped)]\n",
    "\n",
    "# Filter df_mapped to include only valid clusters\n",
    "df_valid = df_mapped[df_mapped['cosine_cluster_icd_title'].isin(valid_clusters)].copy()\n",
    "\n",
    "# Apply the outlier detection to valid clusters\n",
    "df_valid['is_high_outlier'] = df_valid.apply(is_high_outlier, axis=1)\n",
    "\n",
    "# Determine the highest value within each cluster for these outliers\n",
    "highest_outliers = df_valid[df_valid['is_high_outlier']].groupby('cosine_cluster_icd_title')['cosine_distance_to_center_icd_title'].transform('max')\n",
    "\n",
    "# Add 'to_be_removed' column to mark the highest outliers\n",
    "df_valid['to_be_removed'] = df_valid.apply(\n",
    "    lambda row: row['is_high_outlier'] and row['cosine_distance_to_center_icd_title'] == highest_outliers[row.name],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create a cleaned DataFrame without the specific highest outliers\n",
    "df_mapped_cleaned = df_valid[~df_valid['to_be_removed']].copy()\n",
    "\n",
    "# Track affected clusters\n",
    "affected_clusters = df_valid.loc[df_valid['to_be_removed'], 'cosine_cluster_icd_title'].unique()\n",
    "\n",
    "# Drop the extra columns used for outlier detection\n",
    "df_mapped_cleaned.drop(['is_high_outlier', 'to_be_removed'], axis=1, inplace=True)\n",
    "\n",
    "# Print affected clusters\n",
    "print(\"Affected clusters with highest outlier removed:\", affected_clusters)\n",
    "\n",
    "\n",
    "# Verify that all clusters in df_mapped_cleaned have at least 3 unique `icd_title` and `cosine_distance_to_center_icd_title` values\n",
    "cluster_validity_cleaned = df_mapped_cleaned.groupby('cosine_cluster_icd_title').agg({\n",
    "    'icd_title': 'nunique',\n",
    "    'cosine_distance_to_center_icd_title': 'nunique'\n",
    "})\n",
    "\n",
    "invalid_clusters = cluster_validity_cleaned[(cluster_validity_cleaned['icd_title'] < 3) | (cluster_validity_cleaned['cosine_distance_to_center_icd_title'] < 3)].index\n",
    "\n",
    "if len(invalid_clusters) == 0:\n",
    "    print(\"\\nAll clusters in df_mapped_cleaned have at least 3 unique `icd_title` and `cosine_distance_to_center_icd_title` values.\")\n",
    "else:\n",
    "    print(\"\\nClusters violating the rule (less than 3 uniques):\", list(invalid_clusters))\n",
    "\n",
    "# Print change in size before and after cleaning\n",
    "print(f\"\\nSize before cleaning: {df_original.shape[0]}\")\n",
    "print(f\"Size after cleaning: {df_mapped_cleaned.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038c7d9-ddeb-4eb6-bde6-7160f2722d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.to_csv('df_cc-pram_icd-title-original_sample.csv', index=True)\n",
    "df_mapped_cleaned.to_csv('df_cc-pram_icd-title_mapped_cleaned.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ba61c-c064-49e7-9d36-74ade9ed6234",
   "metadata": {},
   "source": [
    "# Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2f0bc-e2d5-4962-81d4-65c08fbed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 14\n",
    "df_valid[df_valid['cosine_cluster_icd_title'] == cluster].sort_values(by='cosine_distance_to_center_icd_title', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f18397-a9d0-410e-ab91-8fc330f9251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cluster\n",
    "df_mapped_cleaned[df_mapped_cleaned['cosine_cluster_icd_title'] == cluster].sort_values(by='cosine_distance_to_center_icd_title', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91333bf-49e9-49f1-9dd7-82168a3e2f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
